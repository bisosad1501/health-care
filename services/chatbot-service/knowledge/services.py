import re
import time
import logging
from typing import List, Dict, Any, Optional, Tuple
from django.db.models import Q, Count
from django.conf import settings
from django.utils import timezone
from fuzzywuzzy import fuzz, process
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

from .models import (
    KnowledgeEntry, KnowledgeCategory, DiseaseInformation, 
    SymptomInformation, MedicalTerm, ChatbotResponse, KnowledgeSearchLog
)

logger = logging.getLogger(__name__)

# Download required NLTK data
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
except:
    pass


class VietnameseTextProcessor:
    """X·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát"""
    
    # Stopwords ti·∫øng Vi·ªát
    VIETNAMESE_STOPWORDS = {
        'v√†', 'c·ªßa', 'v·ªõi', 't·ª´', 'trong', 'tr√™n', 'd∆∞·ªõi', 'v·ªÅ', 'cho', 'ƒë·ªÉ',
        'khi', 'n·∫øu', 'v√¨', 'nh∆∞ng', 'm√†', 'r·ªìi', 'ƒë√£', 's·∫Ω', 'c√≥', 'l√†',
        'ƒë∆∞·ª£c', 'b·ªã', 'c√°c', 'nh·ªØng', 'n√†y', 'ƒë√≥', '·ªü', 't·∫°i', 'theo', 'nh∆∞',
        'ch·ªâ', 'c≈©ng', 'ƒë·ªÅu', 'c·∫£', 'th√¨', 'hay', 'ho·∫∑c', 'n√™n', 'ph·∫£i', 'c·∫ßn',
        'm·ªôt', 'hai', 'ba', 'b·ªën', 'nƒÉm', 's√°u', 'b·∫£y', 't√°m', 'ch√≠n', 'm∆∞·ªùi'
    }
    
    # T·ª´ kh√≥a y t·∫ø ti·∫øng Vi·ªát
    MEDICAL_KEYWORDS = {
        'b·ªánh', 'tri·ªáu ch·ª©ng', 'ƒëi·ªÅu tr·ªã', 'thu·ªëc', 'b√°c sƒ©', 'b·ªánh vi·ªán',
        'kh√°m', 'ch·∫©n ƒëo√°n', 'ph√≤ng ng·ª´a', 's·ª©c kh·ªèe', 'y t·∫ø', 'c·∫•p c·ª©u',
        'ƒëau', 'nh·ª©c', 's·ªët', 'ho', 'vi√™m', 'nhi·ªÖm', 'ung th∆∞', 'tim m·∫°ch'
    }
    
    def __init__(self):
        try:
            self.stemmer = PorterStemmer()
        except:
            self.stemmer = None
    
    def preprocess_text(self, text: str) -> str:
        """Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n"""
        if not text:
            return ""
        
        # Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
        text = text.lower()
        
        # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, gi·ªØ l·∫°i d·∫•u ti·∫øng Vi·ªát
        text = re.sub(r'[^\w\s√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]', ' ', text)
        
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def remove_stopwords(self, text: str) -> str:
        """Lo·∫°i b·ªè stopwords"""
        words = text.split()
        filtered_words = [word for word in words if word not in self.VIETNAMESE_STOPWORDS]
        return ' '.join(filtered_words)
    
    def extract_keywords(self, text: str) -> List[str]:
        """Tr√≠ch xu·∫•t t·ª´ kh√≥a"""
        processed_text = self.preprocess_text(text)
        words = processed_text.split()
        
        # L·ªçc t·ª´ kh√≥a y t·∫ø v√† t·ª´ quan tr·ªçng
        keywords = []
        for word in words:
            if (len(word) > 2 and 
                word not in self.VIETNAMESE_STOPWORDS and
                (word in self.MEDICAL_KEYWORDS or len(word) > 4)):
                keywords.append(word)
        
        return list(set(keywords))


class KnowledgeSearchEngine:
    """C√¥ng c·ª• t√¨m ki·∫øm ki·∫øn th·ª©c"""
    
    def __init__(self):
        self.text_processor = VietnameseTextProcessor()
        self.tfidf_vectorizer = None
        self.document_vectors = None
        self.knowledge_cache = {}
        self.last_cache_update = None
    
    def build_search_index(self):
        """X√¢y d·ª±ng ch·ªâ m·ª•c t√¨m ki·∫øm"""
        try:
            # L·∫•y t·∫•t c·∫£ c√°c entry active
            entries = KnowledgeEntry.objects.filter(is_active=True).select_related('category')
            
            if not entries.exists():
                logger.warning("Kh√¥ng c√≥ knowledge entry n√†o ƒë·ªÉ x√¢y d·ª±ng index")
                return
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu
            documents = []
            entry_ids = []
            
            for entry in entries:
                # K·∫øt h·ª£p title, content, summary v√† keywords
                text_parts = [
                    entry.title,
                    entry.content,
                    entry.summary or '',
                    entry.keywords or '',
                    entry.category.name
                ]
                
                combined_text = ' '.join(filter(None, text_parts))
                processed_text = self.text_processor.preprocess_text(combined_text)
                
                documents.append(processed_text)
                entry_ids.append(entry.id)
            
            # T·∫°o TF-IDF vectors
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=10000,
                ngram_range=(1, 2),
                stop_words=list(self.text_processor.VIETNAMESE_STOPWORDS)
            )
            
            self.document_vectors = self.tfidf_vectorizer.fit_transform(documents)
            
            # Cache entry IDs
            self.knowledge_cache = {
                'entry_ids': entry_ids,
                'documents': documents
            }
            
            self.last_cache_update = time.time()
            logger.info(f"ƒê√£ x√¢y d·ª±ng search index v·ªõi {len(documents)} documents")
            
        except Exception as e:
            logger.error(f"L·ªói khi x√¢y d·ª±ng search index: {str(e)}")
    
    def search_knowledge(self, query: str, limit: int = 10, min_score: float = 0.1) -> List[Dict]:
        """T√¨m ki·∫øm ki·∫øn th·ª©c"""
        start_time = time.time()
        
        try:
            # Ki·ªÉm tra v√† c·∫≠p nh·∫≠t cache
            if (not self.tfidf_vectorizer or 
                not self.last_cache_update or 
                time.time() - self.last_cache_update > 3600):  # C·∫≠p nh·∫≠t m·ªói gi·ªù
                self.build_search_index()
            
            if not self.tfidf_vectorizer:
                return []
            
            # X·ª≠ l√Ω query
            processed_query = self.text_processor.preprocess_text(query)
            query_vector = self.tfidf_vectorizer.transform([processed_query])
            
            # T√≠nh similarity
            similarities = cosine_similarity(query_vector, self.document_vectors).flatten()
            
            # L·∫•y top results
            top_indices = similarities.argsort()[::-1][:limit]
            
            results = []
            for idx in top_indices:
                score = similarities[idx]
                if score >= min_score:
                    entry_id = self.knowledge_cache['entry_ids'][idx]
                    try:
                        entry = KnowledgeEntry.objects.get(id=entry_id)
                        results.append({
                            'entry': entry,
                            'score': float(score),
                            'matched_text': self.knowledge_cache['documents'][idx][:200]
                        })
                    except KnowledgeEntry.DoesNotExist:
                        continue
            
            # Log t√¨m ki·∫øm
            search_duration = time.time() - start_time
            KnowledgeSearchLog.objects.create(
                query=query,
                results_count=len(results),
                search_duration=search_duration
            )
            
            return results
            
        except Exception as e:
            logger.error(f"L·ªói khi t√¨m ki·∫øm: {str(e)}")
            return []
    
    def search_by_category(self, category_type: str, query: str = None, limit: int = 10) -> List[Dict]:
        """T√¨m ki·∫øm theo danh m·ª•c"""
        try:
            # T√¨m trong category
            entries = KnowledgeEntry.objects.filter(
                category__category_type=category_type,
                is_active=True
            ).select_related('category')
            
            if query:
                # T√¨m ki·∫øm fuzzy trong title v√† content
                query_lower = query.lower()
                filtered_entries = []
                
                for entry in entries:
                    title_score = fuzz.partial_ratio(query_lower, entry.title.lower())
                    content_score = fuzz.partial_ratio(query_lower, entry.content.lower())
                    max_score = max(title_score, content_score)
                    
                    if max_score >= 60:  # Ng∆∞·ª°ng similarity
                        filtered_entries.append({
                            'entry': entry,
                            'score': max_score / 100.0,
                            'matched_text': entry.title
                        })
                
                # S·∫Øp x·∫øp theo score
                filtered_entries.sort(key=lambda x: x['score'], reverse=True)
                return filtered_entries[:limit]
            
            return [{'entry': entry, 'score': 1.0, 'matched_text': entry.title} 
                   for entry in entries[:limit]]
            
        except Exception as e:
            logger.error(f"L·ªói khi t√¨m ki·∫øm theo category: {str(e)}")
            return []
        self._initialize_tfidf()
    
    def _initialize_tfidf(self):
        """Kh·ªüi t·∫°o TF-IDF vectorizer"""
        try:
            # L·∫•y t·∫•t c·∫£ n·ªôi dung ƒë·ªÉ training TF-IDF
            entries = KnowledgeEntry.objects.filter(is_active=True)
            documents = []
            
            for entry in entries:
                content = f"{entry.title} {entry.content} {entry.summary or ''} {entry.keywords or ''}"
                processed_content = self.text_processor.preprocess_text(content)
                documents.append(processed_content)
            
            if documents:
                self.tfidf_vectorizer = TfidfVectorizer(
                    max_features=5000,
                    stop_words=list(VietnameseTextProcessor.VIETNAMESE_STOPWORDS),
                    ngram_range=(1, 2)
                )
                self.document_vectors = self.tfidf_vectorizer.fit_transform(documents)
                logger.info(f"TF-IDF initialized with {len(documents)} documents")
        except Exception as e:
            logger.error(f"Error initializing TF-IDF: {str(e)}")
    
    def search_knowledge(self, query: str, filters: Dict[str, Any] = None, limit: int = 10) -> List[Dict]:
        """T√¨m ki·∫øm ki·∫øn th·ª©c"""
        start_time = time.time()
        
        try:
            # Ti·ªÅn x·ª≠ l√Ω query
            processed_query = self.text_processor.preprocess_text(query)
            
            # T√¨m ki·∫øm c∆° b·∫£n
            basic_results = self._basic_search(processed_query, filters, limit * 2)
            
            # T√¨m ki·∫øm semantic n·∫øu c√≥ TF-IDF
            if self.tfidf_vectorizer and self.document_vectors is not None:
                semantic_results = self._semantic_search(processed_query, filters, limit * 2)
                
                # K·∫øt h·ª£p k·∫øt qu·∫£
                combined_results = self._combine_results(basic_results, semantic_results)
            else:
                combined_results = basic_results
            
            # S·∫Øp x·∫øp v√† gi·ªõi h·∫°n k·∫øt qu·∫£
            final_results = self._rank_results(combined_results, query)[:limit]
            
            # Log t√¨m ki·∫øm
            search_duration = time.time() - start_time
            self._log_search(query, len(final_results), search_duration)
            
            return final_results
            
        except Exception as e:
            logger.error(f"Error in knowledge search: {str(e)}")
            return []
    
    def _basic_search(self, query: str, filters: Dict[str, Any], limit: int) -> List[Dict]:
        """T√¨m ki·∫øm c∆° b·∫£n"""
        queryset = KnowledgeEntry.objects.filter(is_active=True)
        
        # √Åp d·ª•ng filters
        if filters:
            if filters.get('category'):
                queryset = queryset.filter(category__name__icontains=filters['category'])
            if filters.get('content_type'):
                queryset = queryset.filter(content_type=filters['content_type'])
            if filters.get('difficulty_level'):
                queryset = queryset.filter(difficulty_level=filters['difficulty_level'])
        
        # T√¨m ki·∫øm full-text
        search_query = (
            Q(title__icontains=query) |
            Q(content__icontains=query) |
            Q(summary__icontains=query) |
            Q(keywords__icontains=query)
        )
        
        entries = queryset.filter(search_query)[:limit]
        
        results = []
        for entry in entries:
            # T√≠nh ƒëi·ªÉm relevance c∆° b·∫£n
            relevance_score = self._calculate_basic_relevance(query, entry)
            
            results.append({
                'entry': entry,
                'score': relevance_score,
                'type': 'basic'
            })
        
        return results
    
    def _semantic_search(self, query: str, filters: Dict[str, Any], limit: int) -> List[Dict]:
        """T√¨m ki·∫øm semantic"""
        try:
            # Vector h√≥a query
            query_vector = self.tfidf_vectorizer.transform([query])
            
            # T√≠nh cosine similarity
            similarities = cosine_similarity(query_vector, self.document_vectors)[0]
            
            # L·∫•y top results
            top_indices = similarities.argsort()[-limit:][::-1]
            
            # L·∫•y entries t∆∞∆°ng ·ª©ng
            entries = list(KnowledgeEntry.objects.filter(is_active=True))
            
            results = []
            for idx in top_indices:
                if idx < len(entries) and similarities[idx] > 0.1:  # Threshold
                    entry = entries[idx]
                    
                    # Ki·ªÉm tra filters
                    if self._match_filters(entry, filters):
                        results.append({
                            'entry': entry,
                            'score': similarities[idx],
                            'type': 'semantic'
                        })
            
            return results
            
        except Exception as e:
            logger.error(f"Error in semantic search: {str(e)}")
            return []
    
    def _calculate_basic_relevance(self, query: str, entry: KnowledgeEntry) -> float:
        """T√≠nh ƒëi·ªÉm relevance c∆° b·∫£n"""
        score = 0.0
        query_lower = query.lower()
        
        # ƒêi·ªÉm cho title
        if query_lower in entry.title.lower():
            score += 3.0
        
        # ƒêi·ªÉm cho keywords
        if entry.keywords and query_lower in entry.keywords.lower():
            score += 2.0
        
        # ƒêi·ªÉm cho summary
        if entry.summary and query_lower in entry.summary.lower():
            score += 1.5
        
        # ƒêi·ªÉm cho content
        if query_lower in entry.content.lower():
            score += 1.0
        
        # Bonus cho reliability
        score *= entry.reliability_score
        
        # Bonus cho verified content
        if entry.is_verified:
            score *= 1.2
        
        return score
    
    def _match_filters(self, entry: KnowledgeEntry, filters: Dict[str, Any]) -> bool:
        """Ki·ªÉm tra entry c√≥ match v·ªõi filters kh√¥ng"""
        if not filters:
            return True
        
        if filters.get('category') and filters['category'].lower() not in entry.category.name.lower():
            return False
        
        if filters.get('content_type') and entry.content_type != filters['content_type']:
            return False
        
        if filters.get('difficulty_level') and entry.difficulty_level != filters['difficulty_level']:
            return False
        
        return True
    
    def _combine_results(self, basic_results: List[Dict], semantic_results: List[Dict]) -> List[Dict]:
        """K·∫øt h·ª£p k·∫øt qu·∫£ t·ª´ c√°c ph∆∞∆°ng ph√°p t√¨m ki·∫øm"""
        combined = {}
        
        # Th√™m basic results
        for result in basic_results:
            entry_id = result['entry'].id
            combined[entry_id] = result
        
        # Th√™m semantic results, k·∫øt h·ª£p score n·∫øu ƒë√£ c√≥
        for result in semantic_results:
            entry_id = result['entry'].id
            if entry_id in combined:
                # K·∫øt h·ª£p score
                combined[entry_id]['score'] = (
                    combined[entry_id]['score'] * 0.6 + result['score'] * 0.4
                )
                combined[entry_id]['type'] = 'combined'
            else:
                combined[entry_id] = result
        
        return list(combined.values())
    
    def _rank_results(self, results: List[Dict], original_query: str) -> List[Dict]:
        """S·∫Øp x·∫øp k·∫øt qu·∫£ theo ƒë·ªô li√™n quan"""
        # T√≠nh to√°n th√™m c√°c y·∫øu t·ªë ranking
        for result in results:
            entry = result['entry']
            
            # Bonus cho view count
            view_bonus = min(entry.view_count / 1000, 0.5)
            result['score'] += view_bonus
            
            # Bonus cho recent updates
            days_since_update = (timezone.now() - entry.updated_at).days
            recency_bonus = max(0, (30 - days_since_update) / 30 * 0.3)
            result['score'] += recency_bonus
        
        # S·∫Øp x·∫øp theo score
        return sorted(results, key=lambda x: x['score'], reverse=True)
    
    def _log_search(self, query: str, results_count: int, duration: float):
        """Log t√¨m ki·∫øm"""
        try:
            KnowledgeSearchLog.objects.create(
                query=query,
                results_count=results_count,
                search_duration=duration
            )
        except Exception as e:
            logger.error(f"Error logging search: {str(e)}")


class SymptomChecker:
    """Ki·ªÉm tra tri·ªáu ch·ª©ng v√† g·ª£i √Ω b·ªánh"""
    
    def __init__(self):
        self.text_processor = VietnameseTextProcessor()
    
    def check_symptoms(self, symptoms: List[str], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Ki·ªÉm tra tri·ªáu ch·ª©ng v√† tr·∫£ v·ªÅ th√¥ng tin li√™n quan"""
        try:
            # Chu·∫©n h√≥a tri·ªáu ch·ª©ng
            normalized_symptoms = [self.text_processor.preprocess_text(s) for s in symptoms]
            
            # T√¨m tri·ªáu ch·ª©ng trong database
            found_symptoms = self._find_symptoms(normalized_symptoms)
            
            # T√¨m b·ªánh li√™n quan
            related_diseases = self._find_related_diseases(found_symptoms)
            
            # ƒê√°nh gi√° m·ª©c ƒë·ªô kh·∫©n c·∫•p
            urgency_level = self._assess_urgency(found_symptoms)
            
            # G·ª£i √Ω h√†nh ƒë·ªông
            recommendations = self._generate_recommendations(found_symptoms, urgency_level, context)
            
            return {
                'symptoms': found_symptoms,
                'related_diseases': related_diseases,
                'urgency_level': urgency_level,
                'recommendations': recommendations,
                'disclaimer': self._get_disclaimer()
            }
            
        except Exception as e:
            logger.error(f"Error in symptom checking: {str(e)}")
            return {
                'error': 'Kh√¥ng th·ªÉ ki·ªÉm tra tri·ªáu ch·ª©ng l√∫c n√†y',
                'disclaimer': self._get_disclaimer()
            }
    
    def _find_symptoms(self, symptoms: List[str]) -> List[Dict]:
        """T√¨m tri·ªáu ch·ª©ng trong database"""
        found_symptoms = []
        
        for symptom in symptoms:
            # T√¨m ki·∫øm fuzzy matching
            all_symptoms = SymptomInformation.objects.filter(is_active=True)
            
            best_match = None
            best_score = 0
            
            for db_symptom in all_symptoms:
                # So s√°nh v·ªõi t√™n tri·ªáu ch·ª©ng
                score = fuzz.partial_ratio(symptom, db_symptom.name.lower())
                
                if score > best_score and score > 70:  # Threshold
                    best_score = score
                    best_match = db_symptom
            
            if best_match:
                found_symptoms.append({
                    'original': symptom,
                    'matched': best_match.name,
                    'info': {
                        'id': str(best_match.id),
                        'name': best_match.name,
                        'description': best_match.description,
                        'urgency_level': best_match.urgency_level,
                        'when_to_see_doctor': best_match.when_to_see_doctor,
                        'home_remedies': best_match.home_remedies
                    },
                    'confidence': best_score / 100
                })
        
        return found_symptoms
    
    def _find_related_diseases(self, symptoms: List[Dict]) -> List[Dict]:
        """T√¨m b·ªánh li√™n quan ƒë·∫øn tri·ªáu ch·ª©ng"""
        if not symptoms:
            return []
        
        # L·∫•y ID c·ªßa c√°c tri·ªáu ch·ª©ng t√¨m ƒë∆∞·ª£c
        symptom_ids = [s['info']['id'] for s in symptoms]
        
        # T√¨m b·ªánh c√≥ li√™n quan ƒë·∫øn c√°c tri·ªáu ch·ª©ng n√†y
        diseases = DiseaseInformation.objects.filter(
            symptoms__id__in=symptom_ids,
            is_active=True
        ).annotate(
            symptom_count=Count('symptoms')
        ).order_by('-symptom_count')[:5]
        
        related_diseases = []
        for disease in diseases:
            # T√≠nh ƒëi·ªÉm match
            match_score = min(disease.symptom_count / len(symptoms), 1.0)
            
            related_diseases.append({
                'id': str(disease.id),
                'name': disease.name,
                'icd_code': disease.icd_code,
                'description': disease.description[:200] + '...' if len(disease.description) > 200 else disease.description,
                'severity_level': disease.severity_level,
                'is_contagious': disease.is_contagious,
                'match_score': match_score,
                'matched_symptoms': disease.symptom_count
            })
        
        return related_diseases
    
    def _assess_urgency(self, symptoms: List[Dict]) -> str:
        """ƒê√°nh gi√° m·ª©c ƒë·ªô kh·∫©n c·∫•p"""
        if not symptoms:
            return 'LOW'
        
        urgency_levels = [s['info']['urgency_level'] for s in symptoms]
        
        if 'EMERGENCY' in urgency_levels:
            return 'EMERGENCY'
        elif 'HIGH' in urgency_levels:
            return 'HIGH'
        elif 'MEDIUM' in urgency_levels:
            return 'MEDIUM'
        else:
            return 'LOW'
    
    def _generate_recommendations(self, symptoms: List[Dict], urgency_level: str, context: Dict) -> List[str]:
        """T·∫°o g·ª£i √Ω h√†nh ƒë·ªông"""
        recommendations = []
        
        if urgency_level == 'EMERGENCY':
            recommendations.append("üö® C·∫ßn c·∫•p c·ª©u ngay l·∫≠p t·ª©c - G·ªçi 115 ho·∫∑c ƒë·∫øn b·ªánh vi·ªán g·∫ßn nh·∫•t")
        elif urgency_level == 'HIGH':
            recommendations.append("‚ö†Ô∏è N√™n g·∫∑p b√°c sƒ© trong v√≤ng 24 gi·ªù")
        elif urgency_level == 'MEDIUM':
            recommendations.append("üìã N√™n ƒë·∫∑t l·ªãch kh√°m b√°c sƒ© trong v√†i ng√†y t·ªõi")
        else:
            recommendations.append("üí° Theo d√µi tri·ªáu ch·ª©ng, c√≥ th·ªÉ t·ª± ƒëi·ªÅu tr·ªã t·∫°i nh√†")
        
        # Th√™m g·ª£i √Ω t·ª´ th√¥ng tin tri·ªáu ch·ª©ng
        for symptom in symptoms:
            if symptom['info'].get('home_remedies'):
                recommendations.append(f"üè† {symptom['info']['home_remedies']}")
        
        # Th√™m g·ª£i √Ω chung
        recommendations.extend([
            "üíß U·ªëng ƒë·ªß n∆∞·ªõc v√† ngh·ªâ ng∆°i",
            "üì± Theo d√µi tri·ªáu ch·ª©ng v√† ghi ch√∫ l·∫°i",
            "ü©∫ Kh√¥ng t·ª± √Ω s·ª≠ d·ª•ng thu·ªëc m√† kh√¥ng c√≥ ch·ªâ ƒë·ªãnh c·ªßa b√°c sƒ©"
        ])
        
        return recommendations[:5]  # Gi·ªõi h·∫°n s·ªë g·ª£i √Ω
    
    def _get_disclaimer(self) -> str:
        """L·∫•y disclaimer"""
        return (
            "‚ö†Ô∏è Th√¥ng tin n√†y ch·ªâ mang t√≠nh ch·∫•t tham kh·∫£o v√† kh√¥ng th·ªÉ thay th·∫ø "
            "vi·ªác kh√°m v√† t∆∞ v·∫•n tr·ª±c ti·∫øp v·ªõi b√°c sƒ©. Khi c√≥ tri·ªáu ch·ª©ng b·∫•t th∆∞·ªùng, "
            "h√£y li√™n h·ªá v·ªõi c∆° s·ªü y t·∫ø ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ch√≠nh x√°c."
        )


class KnowledgeRecommendationEngine:
    """H·ªá th·ªëng g·ª£i √Ω ki·∫øn th·ª©c"""
    
    def __init__(self):
        self.search_engine = KnowledgeSearchEngine()
        self.text_processor = VietnameseTextProcessor()
    
    def get_recommendations(self, user_query: str, context: Dict[str, Any] = None, max_recommendations: int = 5) -> List[Dict]:
        """L·∫•y g·ª£i √Ω ki·∫øn th·ª©c"""
        try:
            # Tr√≠ch xu·∫•t t·ª´ kh√≥a t·ª´ query
            keywords = self.text_processor.extract_keywords(user_query)
            
            # T√¨m ki·∫øm d·ª±a tr√™n t·ª´ kh√≥a
            search_results = self.search_engine.search_knowledge(
                ' '.join(keywords), 
                limit=max_recommendations * 2
            )
            
            # L·ªçc v√† s·∫Øp x·∫øp k·∫øt qu·∫£
            recommendations = self._filter_and_rank_recommendations(
                search_results, user_query, context, max_recommendations
            )
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Error getting recommendations: {str(e)}")
            return []
    
    def _filter_and_rank_recommendations(self, search_results: List[Dict], user_query: str, 
                                       context: Dict, max_recommendations: int) -> List[Dict]:
        """L·ªçc v√† s·∫Øp x·∫øp g·ª£i √Ω"""
        recommendations = []
        
        for result in search_results[:max_recommendations]:
            entry = result['entry']
            
            recommendation = {
                'id': str(entry.id),
                'title': entry.title,
                'summary': entry.summary or entry.content[:200] + '...',
                'category': entry.category.name,
                'content_type': entry.get_content_type_display(),
                'difficulty_level': entry.get_difficulty_level_display(),
                'reliability_score': entry.reliability_score,
                'relevance_score': result['score'],
                'url': f"/api/knowledge/entries/{entry.id}/",
                'why_recommended': self._explain_recommendation(entry, user_query)
            }
            
            recommendations.append(recommendation)
        
        return recommendations
    
    def _explain_recommendation(self, entry: KnowledgeEntry, user_query: str) -> str:
        """Gi·∫£i th√≠ch t·∫°i sao g·ª£i √Ω n√†y ph√π h·ª£p"""
        reasons = []
        
        query_words = self.text_processor.preprocess_text(user_query).split()
        
        # Ki·ªÉm tra t·ª´ kh√≥a trong title
        title_matches = [word for word in query_words if word in entry.title.lower()]
        if title_matches:
            reasons.append(f"Ti√™u ƒë·ªÅ ch·ª©a: {', '.join(title_matches)}")
        
        # Ki·ªÉm tra category
        if any(word in entry.category.name.lower() for word in query_words):
            reasons.append(f"Thu·ªôc danh m·ª•c: {entry.category.name}")
        
        # Ki·ªÉm tra keywords
        if entry.keywords:
            keyword_matches = [word for word in query_words if word in entry.keywords.lower()]
            if keyword_matches:
                reasons.append(f"C√≥ t·ª´ kh√≥a: {', '.join(keyword_matches)}")
        
        if not reasons:
            reasons.append("N·ªôi dung li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n")
        
        return "; ".join(reasons)
