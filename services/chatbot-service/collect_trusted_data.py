#!/usr/bin/env python3
"""
Script thu th·∫≠p d·ªØ li·ªáu t·ª´ c√°c ngu·ªìn y t·∫ø uy t√≠n
CH·∫†Y SCRIPT N√ÄY NGAY ƒë·ªÉ c√≥ d·ªØ li·ªáu th·ª±c t·∫ø!
"""

import requests
import json
import time
import re
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TrustedHealthDataCollector:
    """Thu th·∫≠p d·ªØ li·ªáu t·ª´ ngu·ªìn y t·∫ø uy t√≠n"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Healthcare Knowledge Base Builder 1.0'
        })
        self.collected_data = {
            'categories': [],
            'knowledge_entries': [],
            'diseases': [],
            'symptoms': [],
            'medical_terms': []
        }
    
    def collect_medlineplus_data(self):
        """Thu th·∫≠p t·ª´ MedlinePlus (Public Domain)"""
        logger.info("üîç Thu th·∫≠p d·ªØ li·ªáu t·ª´ MedlinePlus...")
        
        # Danh s√°ch c√°c health topics ph·ªï bi·∫øn
        medlineplus_topics = [
            'https://medlineplus.gov/heartdisease.html',
            'https://medlineplus.gov/highbloodpressure.html', 
            'https://medlineplus.gov/diabetes.html',
            'https://medlineplus.gov/stroke.html',
            'https://medlineplus.gov/cancer.html',
            'https://medlineplus.gov/mentalhealth.html',
            'https://medlineplus.gov/obesity.html',
            'https://medlineplus.gov/flu.html'
        ]
        
        for topic_url in medlineplus_topics:
            try:
                self._extract_medlineplus_topic(topic_url)
                time.sleep(1)  # Respectful crawling
            except Exception as e:
                logger.error(f"L·ªói khi thu th·∫≠p {topic_url}: {e}")
    
    def _extract_medlineplus_topic(self, url):
        """Tr√≠ch xu·∫•t th√¥ng tin t·ª´ m·ªôt topic c·ªßa MedlinePlus"""
        try:
            response = self.session.get(url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # L·∫•y ti√™u ƒë·ªÅ
            title_elem = soup.find('h1')
            if not title_elem:
                return
            
            title = title_elem.text.strip()
            
            # L·∫•y n·ªôi dung ch√≠nh
            main_content = soup.find('div', {'class': 'page-info'})
            if not main_content:
                main_content = soup.find('article')
            
            if not main_content:
                return
            
            # Tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn
            paragraphs = main_content.find_all('p')
            content_parts = []
            
            for p in paragraphs[:5]:  # L·∫•y 5 ƒëo·∫°n ƒë·∫ßu
                text = p.get_text().strip()
                if len(text) > 50:  # B·ªè qua ƒëo·∫°n qu√° ng·∫Øn
                    content_parts.append(text)
            
            if not content_parts:
                return
            
            content = '\\n\\n'.join(content_parts)
            
            # T·∫°o knowledge entry
            knowledge_entry = {
                'title': title,
                'content': content,
                'summary': content_parts[0] if content_parts else '',
                'category': 'Th√¥ng Tin Y T·∫ø',
                'content_type': 'ARTICLE',
                'difficulty_level': 'BASIC',
                'keywords': self._extract_keywords(title + ' ' + content),
                'author': 'MedlinePlus',
                'source': url,
                'reliability_score': 0.95,
                'is_verified': True,
                'tags': ['medlineplus', 'public-health']
            }
            
            self.collected_data['knowledge_entries'].append(knowledge_entry)
            logger.info(f"‚úÖ Thu th·∫≠p th√†nh c√¥ng: {title}")
            
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω {url}: {e}")
    
    def collect_who_data(self):
        """Thu th·∫≠p t·ª´ WHO (Creative Commons License)"""
        logger.info("üîç Thu th·∫≠p d·ªØ li·ªáu t·ª´ WHO...")
        
        # Danh s√°ch c√°c fact sheets c·ªßa WHO
        who_topics = [
            'https://www.who.int/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds)',
            'https://www.who.int/news-room/fact-sheets/detail/hypertension',
            'https://www.who.int/news-room/fact-sheets/detail/diabetes',
            'https://www.who.int/news-room/fact-sheets/detail/obesity-and-overweight',
            'https://www.who.int/news-room/fact-sheets/detail/mental-disorders'
        ]
        
        for topic_url in who_topics:
            try:
                self._extract_who_factsheet(topic_url)
                time.sleep(2)  # Respectful crawling
            except Exception as e:
                logger.error(f"L·ªói khi thu th·∫≠p WHO {topic_url}: {e}")
    
    def _extract_who_factsheet(self, url):
        """Tr√≠ch xu·∫•t WHO fact sheet"""
        try:
            response = self.session.get(url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # L·∫•y ti√™u ƒë·ªÅ
            title_elem = soup.find('h1')
            if not title_elem:
                return
            
            title = title_elem.text.strip()
            
            # L·∫•y key facts
            key_facts = soup.find('div', string=re.compile('Key facts', re.I))
            if key_facts:
                facts_section = key_facts.find_next('ul')
                if facts_section:
                    facts = [li.get_text().strip() for li in facts_section.find_all('li')]
                    
                    knowledge_entry = {
                        'title': f"{title} - Key Facts",
                        'content': '\\n'.join([f"‚Ä¢ {fact}" for fact in facts]),
                        'summary': facts[0] if facts else '',
                        'category': 'Th√¥ng Tin Y T·∫ø WHO',
                        'content_type': 'FACT',
                        'difficulty_level': 'INTERMEDIATE',
                        'keywords': self._extract_keywords(title),
                        'author': 'World Health Organization',
                        'source': url,
                        'reliability_score': 0.98,
                        'is_verified': True,
                        'tags': ['who', 'international-health']
                    }
                    
                    self.collected_data['knowledge_entries'].append(knowledge_entry)
                    logger.info(f"‚úÖ Thu th·∫≠p WHO th√†nh c√¥ng: {title}")
                    
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω WHO {url}: {e}")
    
    def collect_vietnamese_health_data(self):
        """Thu th·∫≠p d·ªØ li·ªáu y t·∫ø Vi·ªát Nam"""
        logger.info("üîç Thu th·∫≠p d·ªØ li·ªáu y t·∫ø Vi·ªát Nam...")
        
        # T·∫°o d·ªØ li·ªáu m·∫´u d·ª±a tr√™n th·ªëng k√™ y t·∫ø VN
        vietnamese_diseases = [
            {
                'name': 'TƒÉng huy·∫øt √°p',
                'icd_code': 'I10',
                'description': 'B·ªánh l√Ω tim m·∫°ch ph·ªï bi·∫øn nh·∫•t t·∫°i Vi·ªát Nam, ·∫£nh h∆∞·ªüng ƒë·∫øn 25.1% d√¢n s·ªë tr∆∞·ªüng th√†nh',
                'causes': 'Di truy·ªÅn, l·ªëi s·ªëng kh√¥ng l√†nh m·∫°nh, ƒÉn nhi·ªÅu mu·ªëi, √≠t v·∫≠n ƒë·ªông, stress',
                'symptoms': 'Th∆∞·ªùng kh√¥ng c√≥ tri·ªáu ch·ª©ng r√µ r√†ng ·ªü giai ƒëo·∫°n ƒë·∫ßu, c√≥ th·ªÉ ƒëau ƒë·∫ßu, ch√≥ng m·∫∑t, m·ªát m·ªèi',
                'treatment': 'Thay ƒë·ªïi l·ªëi s·ªëng, thu·ªëc h·∫° √°p theo ch·ªâ ƒë·ªãnh b√°c sƒ©, theo d√µi th∆∞·ªùng xuy√™n',
                'prevention': 'ƒÇn √≠t mu·ªëi (<5g/ng√†y), t·∫≠p th·ªÉ d·ª•c ƒë·ªÅu ƒë·∫∑n, ki·ªÉm so√°t c√¢n n·∫∑ng, kh√¥ng h√∫t thu·ªëc',
                'category': 'B·ªánh Tim M·∫°ch',
                'severity_level': 'MODERATE',
                'is_chronic': True
            },
            {
                'name': 'Vi√™m gan B',
                'icd_code': 'B18.1',
                'description': 'B·ªánh truy·ªÅn nhi·ªÖm ph·ªï bi·∫øn t·∫°i Vi·ªát Nam, kho·∫£ng 7-8% d√¢n s·ªë mang virus',
                'causes': 'Virus vi√™m gan B (HBV), l√¢y truy·ªÅn qua ƒë∆∞·ªùng m√°u, t√¨nh d·ª•c, t·ª´ m·∫π sang con',
                'symptoms': 'M·ªát m·ªèi, ch√°n ƒÉn, bu·ªìn n√¥n, ƒëau b·ª•ng, v√†ng da, n∆∞·ªõc ti·ªÉu s·∫´m m√†u',
                'treatment': 'Thu·ªëc kh√°ng virus, theo d√µi ch·ª©c nƒÉng gan, ch·∫ø ƒë·ªô ƒÉn u·ªëng ph√π h·ª£p',
                'prevention': 'Ti√™m vaccine, tr√°nh d√πng chung kim ti√™m, quan h·ªá t√¨nh d·ª•c an to√†n',
                'category': 'B·ªánh Truy·ªÅn Nhi·ªÖm',
                'severity_level': 'SEVERE',
                'is_contagious': True
            }
        ]
        
        for disease_data in vietnamese_diseases:
            self.collected_data['diseases'].append(disease_data)
        
        # Tri·ªáu ch·ª©ng ph·ªï bi·∫øn t·∫°i VN
        vietnamese_symptoms = [
            {
                'name': 'S·ªët',
                'description': 'Th√¢n nhi·ªát tƒÉng cao tr√™n 37.5¬∞C, ph·∫£n ·ª©ng c·ªßa c∆° th·ªÉ v·ªõi nhi·ªÖm tr√πng ho·∫∑c b·ªánh l√Ω',
                'body_part': 'To√†n th√¢n',
                'urgency_level': 'MEDIUM',
                'possible_causes': 'Nhi·ªÖm virus, nhi·ªÖm khu·∫©n, s·ªët r√©t, s·ªët xu·∫•t huy·∫øt',
                'when_to_see_doctor': 'S·ªët tr√™n 39¬∞C, s·ªët k√©o d√†i >3 ng√†y, k√®m kh√≥ th·ªü, co gi·∫≠t',
                'home_remedies': 'U·ªëng nhi·ªÅu n∆∞·ªõc, ngh·ªâ ng∆°i, ch∆∞·ªùm m√°t, theo d√µi th√¢n nhi·ªát',
                'category': 'Tri·ªáu Ch·ª©ng Th∆∞·ªùng G·∫∑p'
            },
            {
                'name': 'Ho',
                'description': 'Ph·∫£n x·∫° ƒë·ªÉ lo·∫°i b·ªè ch·∫•t l·∫° kh·ªèi ƒë∆∞·ªùng h√¥ h·∫•p',
                'body_part': 'H·ªá h√¥ h·∫•p',
                'urgency_level': 'LOW',
                'possible_causes': 'C·∫£m l·∫°nh, vi√™m ph·ªïi, d·ªã ·ª©ng, lao ph·ªïi, COVID-19',
                'when_to_see_doctor': 'Ho ra m√°u, ho k√©o d√†i >3 tu·∫ßn, k√®m s·ªët cao, kh√≥ th·ªü',
                'home_remedies': 'U·ªëng n∆∞·ªõc ·∫•m, ngh·ªâ ng·ªçi, tr√°nh kh√≥i b·ª•i, s√∫c h·ªçng n∆∞·ªõc mu·ªëi',
                'category': 'Tri·ªáu Ch·ª©ng Th∆∞·ªùng G·∫∑p'
            }
        ]
        
        for symptom_data in vietnamese_symptoms:
            self.collected_data['symptoms'].append(symptom_data)
    
    def _extract_keywords(self, text):
        """Tr√≠ch xu·∫•t t·ª´ kh√≥a t·ª´ vƒÉn b·∫£n"""
        # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát v√† chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
        clean_text = re.sub(r'[^\\w\\s]', ' ', text.lower())
        words = clean_text.split()
        
        # Lo·∫°i b·ªè stop words v√† l·∫•y t·ª´ kh√≥a quan tr·ªçng
        stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        keywords = [word for word in words if len(word) > 3 and word not in stop_words]
        
        # L·∫•y top 10 t·ª´ kh√≥a duy nh·∫•t
        unique_keywords = list(dict.fromkeys(keywords))[:10]
        return ', '.join(unique_keywords)
    
    def save_collected_data(self, filename='collected_health_data.json'):
        """L∆∞u d·ªØ li·ªáu ƒë√£ thu th·∫≠p"""
        # Th√™m categories c·∫ßn thi·∫øt
        self.collected_data['categories'] = [
            {
                'name': 'Th√¥ng Tin Y T·∫ø',
                'category_type': 'DISEASE',
                'description': 'Th√¥ng tin y t·∫ø t·ª´ ngu·ªìn uy t√≠n qu·ªëc t·∫ø'
            },
            {
                'name': 'Th√¥ng Tin Y T·∫ø WHO', 
                'category_type': 'DISEASE',
                'description': 'Th√¥ng tin t·ª´ T·ªï ch·ª©c Y t·∫ø Th·∫ø gi·ªõi'
            },
            {
                'name': 'B·ªánh Tim M·∫°ch',
                'category_type': 'DISEASE', 
                'description': 'C√°c b·ªánh l√Ω tim m·∫°ch'
            },
            {
                'name': 'B·ªánh Truy·ªÅn Nhi·ªÖm',
                'category_type': 'DISEASE',
                'description': 'C√°c b·ªánh truy·ªÅn nhi·ªÖm'
            },
            {
                'name': 'Tri·ªáu Ch·ª©ng Th∆∞·ªùng G·∫∑p',
                'category_type': 'SYMPTOM',
                'description': 'C√°c tri·ªáu ch·ª©ng th∆∞·ªùng g·∫∑p'
            }
        ]
        
        # Th√™m tags
        self.collected_data['tags'] = [
            {'name': 'medlineplus', 'description': 'Th√¥ng tin t·ª´ MedlinePlus'},
            {'name': 'who', 'description': 'Th√¥ng tin t·ª´ WHO'},
            {'name': 'public-health', 'description': 'Y t·∫ø c√¥ng c·ªông'},
            {'name': 'international-health', 'description': 'Y t·∫ø qu·ªëc t·∫ø'},
            {'name': 'vietnam-health', 'description': 'Y t·∫ø Vi·ªát Nam'}
        ]
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.collected_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üíæ ƒê√£ l∆∞u d·ªØ li·ªáu v√†o {filename}")
        logger.info(f"üìä Th·ªëng k√™ thu th·∫≠p:")
        logger.info(f"   - Categories: {len(self.collected_data['categories'])}")
        logger.info(f"   - Knowledge Entries: {len(self.collected_data['knowledge_entries'])}")
        logger.info(f"   - Diseases: {len(self.collected_data['diseases'])}")
        logger.info(f"   - Symptoms: {len(self.collected_data['symptoms'])}")


def main():
    """Ch·∫°y script thu th·∫≠p d·ªØ li·ªáu"""
    print("üöÄ B·∫ÆT ƒê·∫¶U THU TH·∫¨P KNOWLEDGE BASE T·ª™ NGU·ªíN UY T√çN")
    print("=" * 60)
    
    collector = TrustedHealthDataCollector()
    
    # Thu th·∫≠p t·ª´ c√°c ngu·ªìn
    collector.collect_medlineplus_data()
    collector.collect_who_data() 
    collector.collect_vietnamese_health_data()
    
    # L∆∞u d·ªØ li·ªáu
    collector.save_collected_data('trusted_health_knowledge.json')
    
    print("=" * 60)
    print("‚úÖ HO√ÄN TH√ÄNH! D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'trusted_health_knowledge.json'")
    print("üìù B∆∞·ªõc ti·∫øp theo:")
    print("   1. Review d·ªØ li·ªáu trong file JSON")
    print("   2. Load v√†o database: python manage.py load_knowledge_data --data-file=trusted_health_knowledge.json")
    print("   3. Build search index: python manage.py build_search_index")


if __name__ == "__main__":
    main()
